# ğŸ¤– K-Fix â€” Intelligent Kubernetes Fix Agent

K-Fix est un agent IA intelligent qui reÃ§oit les alertes Datadog et propose des **correctifs Kubernetes automatisÃ©s** (via MR GitLab), avec raisonnement contextuel et apprentissage continu.

---

## ğŸ¯ Objectif

- RÃ©ceptionner les alertes Datadog (CPU, mÃ©moire, erreurs frÃ©quentes).
- Enrichir avec contexte (logs, mÃ©triques, infos k8s).
- Raisonner avec un moteur IA (LLM + rÃ¨gles).
- GÃ©nÃ©rer plusieurs plans de correction (patchs Kubernetes).
- Proposer une Merge Request GitLab justifiÃ©e.
- Apprendre des incidents passÃ©s (Vector DB + feedback).

---

## ğŸš€ Workflow Intelligent

```text
[1] Datadog â†’ webhook
[2] K-Fix reÃ§oit et enrichit alerte
[3] RÃ©cupÃ¨re contexte (metrics, logs, kube API)
[4] Decision Engine (LLM + rÃ¨gles)
[5] GÃ©nÃ¨re actions candidates (patch YAML)
[6] Safety Layer (dry-run, quotas)
[7] CrÃ©e MR GitLab + notif Slack/Teams
[8] Feedback (BDD relationnelle + Vector DB)
```

---

## ğŸ§  Workflow DÃ©taillÃ© du Moteur IA

### ğŸ“¥ Phase 1 : RÃ©ception & Enrichissement

#### 1.1 RÃ©ception Webhook Datadog
```python
# Payload Datadog brut reÃ§u via /datadog-webhook
{
  "alert_id": "12345",
  "monitor_id": "67890",
  "event_type": "triggered",
  "title": "High CPU usage on pod xyz",
  "message": "CPU > 80% for 5 minutes",
  "tags": ["env:prod", "service:api", "namespace:backend"]
}
```

#### 1.2 Enrichissement Multi-Source
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Datadog API   â”‚    â”‚ Kubernetes API  â”‚    â”‚  Vector DB      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Monitor info  â”‚    â”‚ â€¢ Pod status    â”‚    â”‚ â€¢ Incidents     â”‚
â”‚ â€¢ Logs rÃ©cents  â”‚    â”‚ â€¢ Deployment    â”‚    â”‚   similaires    â”‚
â”‚ â€¢ MÃ©triques     â”‚    â”‚ â€¢ Events        â”‚    â”‚ â€¢ Solutions     â”‚
â”‚ â€¢ Historique    â”‚    â”‚ â€¢ Resources     â”‚    â”‚   passÃ©es       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    context_bundle       â”‚
                    â”‚                         â”‚
                    â”‚ {                       â”‚
                    â”‚   "alert": {...},       â”‚
                    â”‚   "monitor": {...},     â”‚
                    â”‚   "k8s_context": {...}, â”‚
                    â”‚   "logs": [...],        â”‚
                    â”‚   "metrics": {...},     â”‚
                    â”‚   "similar_incidents": [â”‚
                    â”‚     {...}               â”‚
                    â”‚   ]                     â”‚
                    â”‚ }                       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¤– Phase 2 : Raisonnement IA

#### 2.1 Recherche d'Incidents Similaires (PRIORITÃ‰)
```python
# Ã‰tape 1: CrÃ©er embedding du context_bundle actuel
current_embedding = create_embedding(context_bundle)

# Ã‰tape 2: Rechercher dans Vector DB
similar_incidents = vector_db.search(
    embedding=current_embedding,
    limit=3,
    threshold=0.8  # SimilaritÃ© minimum
)

# Ã‰tape 3: Enrichir le contexte avec l'historique
enriched_context = {
    **context_bundle,
    "similar_incidents": similar_incidents,
    "lessons_learned": extract_lessons(similar_incidents),
    "successful_patterns": get_successful_solutions(similar_incidents),
    "failed_patterns": get_failed_solutions(similar_incidents)
}
```

```text
Vector Search Process:
1. Convertir context_bundle en embedding
2. Rechercher dans Vector DB (similaritÃ© cosinus > 0.8)
3. RÃ©cupÃ©rer top 3 incidents similaires
4. Extraire patterns de succÃ¨s/Ã©chec
5. Enrichir le contexte pour l'IA
```

#### 2.2 Analyse Contextuelle (LLM avec Historique)
```text
Prompt Engineering Structure:

SYSTEM: Tu es un expert DevOps Kubernetes. Analyse ce context_bundle enrichi et propose des solutions optimisÃ©es.

CONTEXT ACTUEL: 
- Alert: {alert_details}
- K8s State: {k8s_context}
- Recent Logs: {logs_summary}

CONTEXTE HISTORIQUE:
- Similar Past Incidents: {similar_incidents}
- Solutions qui ont FONCTIONNÃ‰: {successful_patterns}
- Solutions qui ont Ã‰CHOUÃ‰: {failed_patterns}
- LeÃ§ons apprises: {lessons_learned}

RULES:
- Jamais d'auto-merge en production
- Dry-run obligatoire
- Respecter les quotas namespace
- Proposer 2-3 alternatives
- Justifier chaque action
- PRIORITÃ‰ aux solutions ayant dÃ©jÃ  rÃ©ussi
- Ã‰VITER les solutions ayant Ã©chouÃ©

TASK: En tenant compte de l'historique, gÃ©nÃ¨re un plan d'action optimisÃ© avec justification basÃ©e sur l'expÃ©rience passÃ©e.
```

#### 2.3 Moteur de RÃ¨gles (Safety Layer)
```python
class SafetyRules:
    def validate_action(self, action_plan, environment, historical_context):
        checks = [
            self.check_environment_policy(environment),
            self.check_resource_quotas(action_plan),
            self.check_scaling_limits(action_plan),
            self.check_dry_run_feasibility(action_plan),
            self.check_historical_failures(action_plan, historical_context)
        ]
        return all(checks)
    
    def check_environment_policy(self, env):
        if env == "production":
            return False  # Jamais d'auto-merge en prod
        return True
    
    def check_historical_failures(self, action_plan, historical_context):
        # VÃ©rifier si cette solution a dÃ©jÃ  Ã©chouÃ© dans le passÃ©
        failed_patterns = historical_context.get('failed_patterns', [])
        for failed_pattern in failed_patterns:
            if self.is_similar_solution(action_plan, failed_pattern):
                return False
        return True
```

#### 2.3 Analyse Contextuelle (LLM)
```text
Prompt Engineering Structure:

SYSTEM: Tu es un expert DevOps Kubernetes. Analyse ce context_bundle et propose des solutions.

CONTEXT: 
- Alert: {alert_details}
- K8s State: {k8s_context}
- Recent Logs: {logs_summary}
- Similar Past Incidents: {vector_search_results}

RULES:
- Jamais d'auto-merge en production
- Dry-run obligatoire
- Respecter les quotas namespace
- Proposer 2-3 alternatives
- Justifier chaque action

TASK: GÃ©nÃ¨re un plan d'action structurÃ© avec justification.
```

#### 2.2 Moteur de RÃ¨gles (Safety Layer)
```python
class SafetyRules:
    def validate_action(self, action_plan, environment):
        checks = [
            self.check_environment_policy(environment),
            self.check_resource_quotas(action_plan),
            self.check_scaling_limits(action_plan),
            self.check_dry_run_feasibility(action_plan)
        ]
        return all(checks)
    
    def check_environment_policy(self, env):
        if env == "production":
            return False  # Jamais d'auto-merge en prod
        return True
```

#### 2.3 Recherche d'Incidents Similaires
```text
Vector Search Process:
1. Convertir context_bundle en embedding
2. Rechercher dans Vector DB (similaritÃ© cosinus)
3. RÃ©cupÃ©rer top 3 incidents similaires
4. Analyser succÃ¨s/Ã©checs des solutions passÃ©es
5. Adapter la stratÃ©gie en consÃ©quence
```

### âš™ï¸ Phase 3 : GÃ©nÃ©ration de Solutions

#### 3.1 Plans d'Action Candidats
```json
{
  "incident_id": "inc_2024_001",
  "analysis": {
    "root_cause": "CPU throttling due to insufficient requests/limits",
    "severity": "high",
    "affected_services": ["api-backend"],
    "estimated_impact": "Response time +200ms"
  },
  "action_plans": [
    {
      "plan_id": "plan_1",
      "title": "Augmenter les ressources CPU",
      "priority": 1,
      "confidence": 0.85,
      "justification": "Logs montrent CPU throttling constant depuis 10min",
      "kubernetes_changes": [
        {
          "resource": "deployment/api-backend",
          "action": "patch",
          "changes": {
            "spec.template.spec.containers[0].resources.requests.cpu": "500m",
            "spec.template.spec.containers[0].resources.limits.cpu": "1000m"
          }
        }
      ],
      "rollback_plan": "Revenir aux valeurs prÃ©cÃ©dentes si CPU usage > 90%",
      "estimated_time": "2-3 minutes",
      "risks": ["Augmentation coÃ»t", "Possible sur-provisioning"]
    },
    {
      "plan_id": "plan_2", 
      "title": "Activer HPA (Horizontal Pod Autoscaler)",
      "priority": 2,
      "confidence": 0.70,
      "justification": "Alternative scaling automatique",
      "kubernetes_changes": [...],
      "rollback_plan": "DÃ©sactiver HPA et revenir au scaling manuel"
    }
  ]
}
```

#### 3.2 Validation Safety Layer
```text
Safety Checks Pipeline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dry-Run Test   â”‚    â”‚  Quota Check    â”‚    â”‚ Policy Engine   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ kubectl apply   â”‚    â”‚ CPU/Memory      â”‚    â”‚ Environment     â”‚
â”‚ --dry-run=serverâ”‚ -> â”‚ limits respect  â”‚ -> â”‚ rules (prod/    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚ staging/dev)    â”‚
â”‚ âœ… Valid YAML   â”‚    â”‚ âœ… Under quota  â”‚    â”‚ âœ… Authorized   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Phase 4 : ExÃ©cution & Collaboration

#### 4.1 CrÃ©ation Merge Request GitLab
```python
class GitLabIntegration:
    def create_mr(self, action_plan):
        mr_content = {
            "title": f"ğŸ¤– K-Fix: {action_plan['title']}",
            "description": self.generate_mr_description(action_plan),
            "source_branch": f"kfix/incident-{action_plan['incident_id']}",
            "target_branch": "main",
            "labels": ["k-fix", "automated", action_plan['severity']],
            "assignee": self.get_team_lead(action_plan['namespace'])
        }
        return self.gitlab_api.create_merge_request(mr_content)
```

#### 4.2 Notification Slack StructurÃ©e
```json
{
  "blocks": [
    {
      "type": "header",
      "text": "ğŸš¨ K-Fix: Incident DÃ©tectÃ© & Solution ProposÃ©e"
    },
    {
      "type": "section",
      "fields": [
        {"type": "mrkdwn", "text": "*Incident:* High CPU usage"},
        {"type": "mrkdwn", "text": "*Service:* api-backend"},
        {"type": "mrkdwn", "text": "*Environnement:* production"},
        {"type": "mrkdwn", "text": "*SÃ©vÃ©ritÃ©:* High"}
      ]
    },
    {
      "type": "section",
      "text": "*ğŸ” Diagnostic:* CPU throttling dÃ©tectÃ©. Requests/limits insuffisants."
    },
    {
      "type": "section",
      "text": "*ğŸ’¡ Solution ProposÃ©e:* Augmenter CPU requests Ã  500m, limits Ã  1000m"
    },
    {
      "type": "actions",
      "elements": [
        {
          "type": "button",
          "text": "ğŸ“‹ Voir MR GitLab",
          "url": "https://gitlab.com/project/merge_requests/123"
        },
        {
          "type": "button", 
          "text": "ğŸ“Š Diagnostic JSON",
          "url": "https://k-fix.internal/incidents/inc_2024_001"
        }
      ]
    }
  ]
}
```

### ğŸ“š Phase 5 : Apprentissage & Feedback

#### 5.1 Stockage Vector DB
```python
class LearningEngine:
    def store_incident(self, context_bundle, action_plan, outcome):
        # CrÃ©er embedding du contexte
        embedding = self.create_embedding(context_bundle)
        
        # Stocker dans Vector DB
        incident_record = {
            "embedding": embedding,
            "context": context_bundle,
            "solution": action_plan,
            "outcome": outcome,  # success/failure/partial
            "feedback": None,    # Ã€ remplir par l'Ã©quipe
            "timestamp": datetime.now()
        }
        
        self.vector_db.store(incident_record)
```

#### 5.2 Boucle de Feedback
```text
Feedback Loop:
1. MR mergÃ©e/rejetÃ©e â†’ outcome capturÃ©
2. Monitoring post-dÃ©ploiement (15min)
3. MÃ©triques collectÃ©es (CPU, erreurs, latence)
4. Ã‰quipe peut ajouter feedback manuel
5. Mise Ã  jour Vector DB avec rÃ©sultat
6. AmÃ©lioration prompts LLM pour cas similaires
```

### ğŸ¯ RÃ©sultat Final

#### Output JSON Complet
```json
{
  "incident_id": "inc_2024_001",
  "timestamp": "2024-01-15T10:30:00Z",
  "status": "solution_proposed",
  "context_bundle": { /* contexte enrichi */ },
  "ai_analysis": { /* diagnostic IA */ },
  "recommended_action": { /* plan choisi */ },
  "alternative_actions": [ /* autres options */ ],
  "safety_checks": { /* validations passÃ©es */ },
  "gitlab_mr": {
    "url": "https://gitlab.com/project/merge_requests/123",
    "branch": "kfix/incident-inc_2024_001"
  },
  "slack_notification": {
    "channel": "#devops-alerts",
    "message_ts": "1705312200.123456"
  },
  "learning": {
    "similar_incidents_found": 2,
    "confidence_score": 0.85,
    "vector_db_stored": true
  }
}
```

---

## âœ… Roadmap de DÃ©veloppement

### Phase 1 â€” Fondations
- [X] CrÃ©er API FastAPI `/datadog-webhook`.
- [X] Parser payload Datadog â†’ `enriched_alert` (namespace, pod, deployment, metric, value, threshold).
- [X] Charger secrets via `.env` (local) ou k8s Secrets.
- [X] Logs structurÃ©s (JSON).

### Phase 2 â€” Contexte & Enrichissement
- [ ] Connecter API Kubernetes (pod, deployment, quotas, HPA).
- [ ] RÃ©cupÃ©rer logs rÃ©cents via Datadog Logs API.
- [ ] Ajouter mÃ©triques mÃ©moire + erreurs applicatives.
- [ ] Construire `context_bundle` complet.

### Phase 3 â€” MÃ©moire & Apprentissage
- [ ] IntÃ©grer Vector DB (Weaviate, Pinecone, Qdrantâ€¦).
- [ ] Stocker incidents + actions proposÃ©es + rÃ©sultat.
- [ ] Activer recherche incidents similaires.
- [ ] CrÃ©er embeddings du `context_bundle`.

### Phase 4 â€” Decision Engine
- [ ] IntÃ©grer un LLM (Claude, GPT, Mistral).
- [ ] Ã‰crire prompts structurÃ©s pour raisonnement.
- [ ] Ajouter rÃ¨gles (policy engine) :
  - jamais auto-merge en prod,
  - dry-run obligatoire,
  - seuil max scaling.

### Phase 5 â€” Actions Candidates
- [ ] GÃ©nÃ©rateur de correctifs Kubernetes (YAML templates).
- [ ] Proposer plusieurs plans dâ€™action adaptÃ©s au type dâ€™alerte reÃ§u.
- [ ] Chaque plan doit inclure :
  - Justification (logs + mÃ©triques + contexte)
  - Alternatives possibles
  - Plan de rollback
- [ ] Lâ€™agent doit rester extensible : intÃ©grer de nouveaux scÃ©narios en fonction
      des incidents passÃ©s (apprentissage via Vector DB + feedback humain).

### Phase 6 â€” Safety Layer
- [ ] ImplÃ©menter `kubectl apply --dry-run=server`.
- [ ] VÃ©rifier quotas namespace.
- [ ] VÃ©rifier cohÃ©rence `request â‰¤ limit`.
- [ ] Rejeter plan si check Ã©choue.

### Phase 7 â€” MR & Collaboration
- [ ] IntÃ©grer GitLab API â†’ crÃ©ation MR.
- [ ] MR contient patch, justification, alternatives, rollback plan.
- [ ] Notifier Slack/Teams avec rÃ©sumÃ© clair.
- [ ] Ajouter labels/tags (env, team).

### Phase 8 â€” Feedback & Learning
- [ ] Relier MR â†” Incident en BDD relationnelle.
- [ ] Suivre statut MR (merged, closed, rejected).
- [ ] Stocker rÃ©sultat dans Vector DB.
- [ ] Adapter prompts LLM en fonction du feedback.

---

## ğŸ§­ Principes de Conception

- **Pas dâ€™automatisme aveugle** : K-Fix ne pousse jamais un correctif sans justification claire.
- **Contexte dâ€™abord** : chaque proposition sâ€™appuie sur les mÃ©triques, logs et Ã©tat du cluster.
- **ExplicabilitÃ©** : toutes les actions candidates sont accompagnÃ©es de leur justification et dâ€™alternatives.
- **SÃ©curitÃ©** : dry-run obligatoire, politiques de quotas respectÃ©es, jamais dâ€™auto-merge en production.
- **Ã‰volutivitÃ©** : lâ€™agent apprend de chaque incident et enrichit sa mÃ©moire (Vector DB + feedback).
- **Collaboration** : les humains restent dans la boucle, K-Fix propose mais ne dÃ©cide pas seul.

---

## ğŸ“Œ Notes

- âš ï¸ **Production** : MR jamais auto-mergÃ©e.  
- âœ… **Staging/Dev** : auto-merge possible.  
- ğŸ”„ Apprentissage continu via Vector DB.  
- ğŸ§  **LLM seul** = Intelligence gÃ©nÃ©rale  
- ğŸ¯ **LLM + Vector DB** = Intelligence spÃ©cialisÃ©e sur VOTRE infrastructure  

---

## ğŸ—‚ Structure prÃ©vue

```
kfix/
 â”œâ”€â”€ main.py          # API FastAPI (webhook Datadog)
 â”œâ”€â”€ context/         # modules de rÃ©cupÃ©ration contexte
 â”œâ”€â”€ decision/        # moteur de raisonnement (LLM + rÃ¨gles)
 â”œâ”€â”€ actions/         # gÃ©nÃ©rateur de patchs Kubernetes
 â”œâ”€â”€ safety/          # validations dry-run, quotas
 â”œâ”€â”€ gitlab/          # interaction GitLab (MR)
 â”œâ”€â”€ memory/          # Vector DB + BDD relationnelle
 â””â”€â”€ tests/           # tests unitaires & intÃ©gration
```

---

## ğŸ›  Tech Stack

- **FastAPI** â†’ serveur API.  
- **Python** â†’ langage principal.  
- **Datadog API** â†’ alertes + logs/metrics.  
- **Kubernetes Python client** â†’ Ã©tat cluster.  
- **GitLab API** â†’ MR.  
- **Vector DB** â†’ mÃ©moire (Weaviate, Pinecone, Qdrant).  
- **Postgres** â†’ base relationnelle incidents â†” MR.  
- **LLM** â†’ raisonnement (Claude, GPT, Mistral).  

---





1. RÃ©ception & Enrichissement multi-source
	â€¢	Recevoir le webhook natif Datadog (alerte brute).
	â€¢	Appeler API Datadog â†’ rÃ©cupÃ©rer monitor_details (tags, query, seuils, groupes).
	â€¢	Appeler API Kubernetes â†’ rÃ©cupÃ©rer pod, deployment, namespace, events.
	â€¢	Optionnel : rÃ©cupÃ©rer logs et mÃ©triques supplÃ©mentaires.
ğŸ‘‰ Objectif : reconstituer un context_bundle complet sans dÃ©pendre du JSON custom.

2. Normalisation du contexte
	â€¢	Transformer le context_bundle dans une forme stable :

{
  "alert": {...},
  "monitor": {...},
  "k8s_context": {...},
  "logs": [...],
  "metrics": {...}
}

	â€¢	Peu importe si lâ€™alerte est CPU, OOM ou CrashLoopBackOff â†’ la structure reste la mÃªme.
ğŸ‘‰ Objectif : rendre les donnÃ©es exploitables pour le moteur de raisonnement.


